---
title: "Sonya Massey Article Analysis"
author: 
  - "Kade Davis"
  - "Morehouse College"
output:
  html_document:
    toc: true
    toc_depth: 3
    toc_float: true  # Enable floating TOC in the sidebar
    number_sections: true
    theme: cerulean
    always_allow_html: true
    self_contained: true
editor_options:
  chunk_output_type: console
  markdown:
    wrap: sentence
---

```{r setup, include=F}
knitr:: opts_chunk$set(echo = TRUE)
```

# Libraries and packages

Load the libraries we needed. Be sure to `install.packages()` where needed.

```{r, message=F, warning=F}
#install.packages("rvest", repos = "https://cloud.r-project.org")
library(pdftools)
library(quanteda)
library(readtext)
library(stringr)
library(rvest)
library(ggplot2)
library(dplyr)
library(tidyverse)
library(tidycensus)
library(dplyr)
library(quanteda.textmodels)
library(ggplot2)
library(sf)
library(viridis)
library(mapview)
library(maps)
library(quanteda.textplots)
library(quanteda.textstats)
```
# DATA

## CNN
### CNN 1 Article

#### Import Article 1 CNN data

```{r, message=F, warning=F}
sonya_cnn1 <- "https://www.cnn.com/2024/07/22/us/sonya-massey-police-shooting" 
sonyacnn1page <- read_html(sonya_cnn1) 
text_data_cnn1 <- html_text(html_nodes(sonyacnn1page, "p")) # Extract all text within the <p> tags
print(text_data_cnn1)  #Print the extracted text
```

#### Generate corpus for Article 1 CNN 

```{r, message=F, warning=F}
corpus_sonyacnn1 <- corpus(text_data_cnn1)
corpus_sonyacnn1 <- # REMOVING LAST THREE TEXTS DUE TO CONTENT
  corpus_subset(corpus_sonyacnn1, 
                !(docnames(corpus_sonyacnn1) %in% c(paste("text",60:62,sep=""))))
tail(corpus_sonyacnn1)
head(corpus_sonyacnn1)
head(corpus_sonyacnn1)
print(corpus_sonyacnn1)
summary(corpus_sonyacnn1)
head(docvars(corpus_sonyacnn1))
```

#### Generate tokens for Article 1 CNN

```{r, message=F, warning=F}
sonya_tokencnn1 <- tokens(corpus_sonyacnn1) #tokenized the pdf text
sonya_nonpunc_cnn1 <- tokens(text_data_cnn1, remove_punct = TRUE) #remove punctuation
print(sonya_nonpunc_cnn1)
```

#### Remove Punctuation, Articles, and Custom Words

```{r, message=F, warning=F}
sonya_nostop_cnn1 <- tokens_select(sonya_nonpunc_cnn1, pattern = stopwords("en"), selection = "remove") #remove articles
print(sonya_nostop_cnn1)
cnn_custom_stop_words <- c("said", "says")
sonya_custom_cnn1 <- tokens_select(sonya_nonpunc_cnn1, pattern = c((stopwords("en")),(cnn_custom_stop_words)), selection = "remove") #remove custom words
print(sonya_custom_cnn1)
```

#### Dataframe

```{r, message=F, warning=F}
sonya.dfm.cnn1 <- dfm(sonya_nostop_cnn1) #put the text into a dataframe matrix
sonya.cnn1.frequency <- topfeatures(sonya.dfm.cnn1, n = 10) #asked for the top 10 words in article
print(sonya.cnn1.frequency)
sonya.cnn1.dfm.custom <- dfm(sonya_custom_cnn1)
sonya.cnn1.frequency.custom <- topfeatures(sonya.cnn1.dfm.custom, n = 10)
print(sonya.cnn1.frequency.custom)
```

#### Table

```{r, message=F, warning=F}
dataframe.cnn1 <- data.frame(sonya.cnn1.frequency.custom)
colnames(dataframe.cnn1) <- "frequency"
#view(dataframe.cnn1)
dataframe.cnn1.organized <- tibble::rownames_to_column(dataframe.cnn1, var = "topwords")
#view(dataframe.cnn1.organized)
dataframe.cnn1.categories <- dataframe.cnn1.organized %>%
  mutate(Group = case_when(
    topwords %in% c("massey", "water", "pot", "black","woman","unarmed","boiling","boil","sonya") ~ "victim",  # For A, B, C -> "first"
    topwords %in% c("grayson", "officer", "deputy","police","deputy","enforcement","officers","deputies","cop","cops","violated","policy") ~ "officer",   # For X, Y, Z -> "last"
    TRUE ~ "other"                            # Default case
  )) 
#view(dataframe.cnn1.categories)
filter.cnn1.categories <- filter(dataframe.cnn1.categories) #filter variables
#filter.cnn1.categories 
#View(filter.cnn1.categories)
```

#### Choose keywords to analyze

```{r, message=F, warning=F}
kw_water_cnn1 <- kwic(sonya_tokencnn1, pattern =  "water") #searching for the word in association with other words
print(kw_water_cnn1)
kw_justification_cnn1 <- kwic(sonya_tokencnn1, pattern = phrase("boiling water*")) #phrase meaning
print(kw_justification_cnn1)
sonya_nostop_cnn1 <- tokens_select(sonya_nonpunc_cnn1, pattern = stopwords("en"), selection = "remove") #remove articles
print(sonya_nostop_cnn1)
sonya_threat_cnn1 <- tokens_select(sonya_nostop_cnn1, pattern = c("speak*","deputy*"), padding = TRUE, window = 5) #view words associated with key words
print(sonya_threat_cnn1)
```

### CNN 2 Article

#### Import Article 2 CNN Data
```{r, message=F, warning=F}
sonya_cnn2 <- "https://www.cnn.com/2024/07/23/us/sonya-massey-police-shooting-what-went-wrong/index.html" 
sonyacnn2page <- read_html(sonya_cnn2) 
text_data_cnn2 <- html_text(html_nodes(sonyacnn2page, "p")) # Extract all text within the <p> tags
print(text_data_cnn2)  #Print the extracted text
```

#### Generate Corpus for Article 2 

```{r, message=F, warning=F}
corpus_sonyacnn2 <- corpus(text_data_cnn2)
corpus_sonyacnn2 <- # REMOVING LAST THREE TEXTS DUE TO CONTENT
  corpus_subset(corpus_sonyacnn2, 
                !(docnames(corpus_sonyacnn2) %in% c("text57", "text58")))
tail(corpus_sonyacnn2)
print(corpus_sonyacnn2)
summary(corpus_sonyacnn2)
head(docvars(corpus_sonyacnn2))
```

#### Generate Tokens for Article 2

```{r, message=F, warning=F}
sonya_tokencnn2 <- tokens(corpus_sonyacnn2) #tokenized the pdf text
sonya_nonpunc_cnn2 <- tokens(sonya_tokencnn2, remove_punct = TRUE) #remove punctuation
print(sonya_nonpunc_cnn2)
```

#### Remove Punctuation, Articles, and Custom Words

```{r, message=F, warning=F}
sonya_nostop_cnn2 <- tokens_select(sonya_nonpunc_cnn2, pattern = stopwords("en"), selection = "remove") #remove articles
print(sonya_nostop_cnn2)
cnn_custom_stop_words <- c("said", "says")
sonya_custom_cnn2 <- tokens_select(sonya_nonpunc_cnn2, pattern = c((stopwords("en")),(cnn_custom_stop_words)), selection = "remove") #remove custom words
print(sonya_custom_cnn2)
```

#### Create Dataframe

```{r, message=F, warning=F}
sonya.dfm.cnn2 <- dfm(sonya_nostop_cnn2) #put the text into a dataframe matrix
sonya.cnn2.frequency <- topfeatures(sonya.dfm.cnn2, n = 10) #asked for the top 10 words in article
print(sonya.cnn2.frequency)
sonya.cnn2.dfm.custom <- dfm(sonya_custom_cnn2)
sonya.cnn2.frequency.custom <- topfeatures(sonya.cnn2.dfm.custom, n = 10)
print(sonya.cnn2.frequency.custom)
```

#### Table

```{r, message=F, warning=F}
dataframe.cnn2 <- data.frame(sonya.cnn2.frequency.custom)
colnames(dataframe.cnn2) <- "frequency"
#view(dataframe.cnn2)
dataframe.cnn2.organized <- tibble::rownames_to_column(dataframe.cnn2, var = "topwords")
#view(dataframe.cnn2.organized)
dataframe.cnn2.categories <- dataframe.cnn2.organized %>%
  mutate(Group = case_when(
    topwords %in% c("massey", "water", "pot", "black","woman","unarmed","boiling","boil","sonya") ~ "victim",  # For A, B, C -> "first"
    topwords %in% c("grayson", "officer", "deputy","police","deputy","enforcement","officers","deputies","cop","cops","violated","policy") ~ "officer",   # For X, Y, Z -> "last"
    TRUE ~ "other"                            # Default case
  )) 
#view(dataframe.cnn2.categories)
filter.cnn2.categories <- filter(dataframe.cnn2.categories) #filter variables
#filter.cnn2.categories 
#View(filter.cnn2.categories)
```

#### Choose Keywords

```{r, message=F, warning=F}
kw_water_cnn2 <- kwic(sonya_tokencnn2, pattern =  "water") #searching for the word in association with other words
print(kw_water_cnn2)
kw_rebuke_cnn2 <- kwic(sonya_tokencnn2, pattern =  "rebuke")
print(kw_rebuke_cnn2)
kw_justification_cnn2 <- kwic(sonya_tokencnn2, pattern = phrase("boiling water*")) #phrase meaning
print(kw_justification_cnn2)
sonya_threat_cnn2 <- tokens_select(sonya_nostop_cnn2, pattern = c("speak*","deputy*"), padding = TRUE, window = 5) #view words associated with key words
print(sonya_threat_cnn2)
```

## FOX
### FOX 1 Article

#### Import Article 1 Fox data

```{r, message=F, warning=F}
sonya_fox1 <- "https://www.foxnews.com/us/bodycam-video-reveals-chaotic-scene-deputy-fatally-shooting-sonya-massey-called-911-help" 
sonya_fox1_page <- read_html(sonya_fox1) 
text_data_fox1 <- html_text(html_nodes(sonya_fox1_page, "p")) # Extract all text within the <p> tags
print(text_data_fox1)
```

#### Generate corpus for Article 1 Fox 

```{r, message=F, warning=F}
corpus_sonyafox1 <- corpus(text_data_fox1)
corpus_sonyafox1 <- # REMOVING LAST THREE TEXTS DUE TO CONTENT
  corpus_subset(corpus_sonyafox1, 
                !(docnames(corpus_sonyafox1) %in% c(paste("text",33:42,sep=""))))
tail(corpus_sonyafox1)
head(corpus_sonyafox1)
print(corpus_sonyafox1)
summary(corpus_sonyafox1)
head(docvars(corpus_sonyafox1))
```

#### Generate tokens for Article 1 Fox

```{r, message=F, warning=F}
sonya_token_fox1 <- tokens(corpus_sonyafox1) #tokenized the pdf text
sonya_nonpunc_fox1 <- tokens(text_data_fox1, remove_punct = TRUE) #remove punctuation
print(sonya_nonpunc_fox1)

```

#### Remove Punctuation, Articles, and Custom Words

```{r, message=F, warning=F}
sonya_nostop_fox1 <- tokens_select(sonya_nonpunc_fox1, pattern = stopwords("en"), selection = "remove") #remove articles
print(sonya_nostop_fox1)
fox_custom_stop_words <- c("said", "says")
sonya_custom_fox1 <- tokens_select(sonya_nonpunc_fox1, pattern = c((stopwords("en")),(fox_custom_stop_words)), selection = "remove") #remove custom words
print(sonya_custom_fox1)
```

#### Dataframe

```{r, message=F, warning=F}
sonya.dfm.fox1 <- dfm(sonya_nostop_fox1) #put the text into a dataframe matrix
sonya.fox1.frequency <- topfeatures(sonya.dfm.fox1, n = 10) #asked for the top 10 words in article
print(sonya.fox1.frequency)
sonya.fox1.dfm.custom <- dfm(sonya_custom_fox1)
sonya.fox1.frequency.custom <- topfeatures(sonya.fox1.dfm.custom, n = 10)
print(sonya.fox1.frequency.custom)
```

#### Table

```{r, message=F, warning=F}
dataframe.fox1 <- data.frame(sonya.fox1.frequency.custom)
colnames(dataframe.fox1) <- "frequency"
#view(dataframe.fox1)
dataframe.fox1.organized <- tibble::rownames_to_column(dataframe.fox1, var = "topwords")
#view(dataframe.fox1.organized)
dataframe.fox1.categories <- dataframe.fox1.organized %>%
  mutate(Group = case_when(
    topwords %in% c("massey", "water", "pot", "black","woman","unarmed","boiling","boil","sonya") ~ "victim",  # For A, B, C -> "first"
    topwords %in% c("grayson", "officer", "deputy","police","deputy","enforcement","officers","deputies","cop","cops","violated","policy") ~ "officer",   # For X, Y, Z -> "last"
    TRUE ~ "other"                            # Default case
  )) 
#view(dataframe.fox1.categories)
filter.fox1.categories <- filter(dataframe.fox1.categories) #filter variables
#filter.fox1.categories 
#View(filter.fox1.categories)
```

#### Semantic Analysis

```{r, message=F, warning=F}
fox_custom_key_words <- c("sonya","massey","grayson","officer","kill*","justified*","just*","fault*","cop*","officer*","violation*","policy")
sonya_key_custom_fox1 <- tokens_select(sonya_nostop_fox1, pattern = fox_custom_key_words) #choose keywords to analyze
sonya.dfm.fox1.key <- dfm(sonya_key_custom_fox1)
sonya.fox1.lsa <- textmodel_lsa(sonya.fox1.dfm.custom)
sonya.fox1.lsa$matrix_low_rank
sonya.fox1.lsa$features
dataframe.fox1.lsa.features <- data.frame(sonya.fox1.lsa$features)
view(dataframe.fox1.lsa.features)
barplot(sonya.fox1.lsa$features)
predict.fox1 <- predict(sonya.fox1.lsa)
predict.fox1
textplot_network(sonya.dfm.fox1.key)
```

#### Choose keywords to analyze

```{r, message=F, warning=F}
kw_water_fox1 <- kwic(sonya_token_fox1, pattern =  "water") #searching for the word in association with other words
print(kw_water_fox1)
kw_rebuke_fox1 <- kwic(sonya_token_fox1, pattern =  "rebuke")
print(kw_rebuke_fox1)
kw_justification_fox1 <- kwic(sonya_token_fox1, pattern = phrase("boiling water*")) #phrase meaning
print(kw_justification_fox1)
sonya_nostop_fox1 <- tokens_select(sonya_nonpunc_fox1, pattern = stopwords("en"), selection = "remove") #remove articles
print(sonya_nostop_fox1)
sonya_threat_fox1 <- tokens_select(sonya_nostop_fox1, pattern = c("speak*","deputy*"), padding = TRUE, window = 5) #view words associated with key words
print(sonya_threat_fox1)
```


### FOX 1 Comments (n=60)
#### Import FOX 1 Comments data

```{r, message=F, warning=F}
sonya_fox1_comments <- readtext("C:/Users/kadej/Dropbox/sonya.quanteda/fox1comments.txt") 
str(sonya_fox1_comments)
sonya_fox1_comments_raw <- sonya_fox1_comments$text
str(sonya_fox1_comments_raw)
chunks <- strsplit(sonya_fox1_comments_raw, "\nReply")[[1]]
fox1_chunks <- trimws(chunks)
fox1_chunks
fox1_cleaned_chunks <- str_extract(fox1_chunks, "[^\n]+$")
fox1_cleaned_chunks
library(stringr)
```

#### Generate corpus for Fox 1 Comments 

```{r, message=F, warning=F}
corpus_sonya_fox1_comments <- corpus(fox1_cleaned_chunks) #make corpus
print(corpus_sonya_fox1_comments)
summary(corpus_sonya_fox1_comments)
head(corpus_sonya_fox1_comments)
```

#### Create tokens for Fox 1 comments and Remove Punctuation, Numbers, and Comments

```{r, message=F, warning=F}
sonya_token_fox1_comments <- tokens(corpus_sonya_fox1_comments) #tokenized the pdf text
sonya_nonpunc_fox1_comments <- tokens(corpus_sonya_fox1_comments, remove_punct = TRUE, remove_numbers = TRUE) #remove punctuation and numbers
print(sonya_nonpunc_fox1_comments)
fox1_remove <- c("See more","Reply")
sonya_nostop_fox1_comments <- tokens_select(sonya_nonpunc_fox1_comments, pattern = c(stopwords("en"), fox1_remove), selection = "remove") #remove articles
print(sonya_nostop_fox1_comments)
```

#### Remove Custom Words

```{r, message=F, warning=F}
#fox1_comments_custom_stop_words <- c("see","reply","july","replies", "share","show","back","years","like")
#sonya_custom_fox1_comments <- tokens_select(sonya_remove_fox1_comments, pattern = c((stopwords("en")),(fox1_comments_custom_stop_words)), selection = "remove") #remove custom words
#print(sonya_custom_fox1_comments)
```

#### Dataframe

```{r, message=F, warning=F}
sonya.dfm.fox1.comments <- dfm(sonya_nostop_fox1_comments) #put the text into a dataframe matrix
sonya.fox1.comments.frequency <- topfeatures(sonya.dfm.fox1.comments, n = 25) #asked for the top 10 words in article
print(sonya.fox1.comments.frequency)
print(sonya.fox1.comments.frequency)
```

#### Table

```{r, message=F, warning=F}
dataframe.fox1.comments <- data.frame(sonya.fox1.comments.frequency)
colnames(dataframe.fox1.comments) <- "frequency"
#view(dataframe.fox1.comments)
dataframe.fox1.comments.organized <- tibble::rownames_to_column(dataframe.fox1.comments, var = "topwords")
#view(dataframe.fox1.comments.organized)
dataframe.fox1.comments.categories <- dataframe.fox1.comments.organized %>%
  mutate(Group = case_when(
    topwords %in% c("massey", "water", "pot", "black","woman","unarmed","boiling","boil","sonya","she") ~ "victim",  # For A, B, C -> "first"
    topwords %in% c("grayson", "officer", "deputy","police","deputy","enforcement","officers","deputies","cop","cops","violated","policy","sheriff","sheriff's","blue") ~ "officer",   # For X, Y, Z -> "last"
    TRUE ~ "other"                            # Default case
  )) 
#view(dataframe.fox1.comments.categories)
filter.fox1.comments.categories <- filter(dataframe.fox1.comments.categories) #filter variables
filter.fox1.comments.categories 
#View(filter.fox1.comments.categories)
```

####semantic analysis 

```{r, message=F, warning=F}
sonya.dfm.fox1.comments <- dfm(sonya_nostop_fox1_comments)
sonya.dfm.fox1.comments
sonya.fox1.comments.lsa <- textmodel_lsa(sonya.dfm.fox1.comments)
sonya.fox1.comments.lsa$matrix_low_rank
sonya.fox1.comments.lsa$features
dataframe.fox1.comments.lsa.features <- data.frame(sonya.fox1.comments.lsa$features)
view(dataframe.fox1.comments.lsa.features)
barplot(sonya.fox1.comments.lsa$features)
network.sonya.comments.fox1 <- textplot_network(sonya.dfm.fox1.comments)
View(sonya.dfm.fox1.comments)
sonya.fox1.comments.textstat <- quanteda.textstats::textstat_keyness(sonya.dfm.fox1.comments)
textplot_keyness(sonya.fox1.comments.textstat, show_reference = TRUE, show_legend = TRUE, n=20L, min_count = 2L, margin = 0.05, color = c("gray","darkblue"), labelcolor = "gray30", labelsize = 4, font = NULL)
fox_custom_key_words <- c("sonya","massey","grayson","officer","kill*","justified*","just*","fault*","cop*","officer*","violation*","policy","evidence*","character*","value*","interpret*","she*","water","pot","boil*")
sonya_key_custom_fox1_comments <- tokens_select(sonya_nostop_fox1_comments, pattern = fox_custom_key_words) #choose keywords to analyze
sonya.dfm.fox1.comments.key <- dfm(sonya_key_custom_fox1_comments)
textplot_network(sonya.dfm.fox1.comments.key, n=40)
```

#### Choose keywords to analyze

```{r, message=F, warning=F}
kw_water_fox1_comments <- kwic(sonya_token_fox1_comments, pattern =  "water") #searching for the word in association with other words
print(kw_water_fox1_comments)
kw_rebuke_fox1_comments <- kwic(sonya_token_fox1_comments, pattern =  "rebuke")
print(kw_rebuke_fox1_comments)
kw_justification_fox1_comments <- kwic(sonya_token_fox1_comments, pattern = phrase("boiling water*")) #phrase meaning
print(kw_justification_fox1_comments)
sonya_nostop_fox1_comments <- tokens_select(sonya_nonpunc_fox1_comments, pattern = stopwords("en"), selection = "remove") #remove articles
print(sonya_nostop_fox1)
sonya_threat_fox1_comments <- tokens_select(sonya_nostop_fox1_comments, pattern = c("speak*","deputy*"), padding = TRUE, window = 5) #view words associated with key words
print(sonya_threat_fox1_comments)
```

### Fox 2 Article

#### Import Fox 2 Article Data

```{r, message=F, warning=F}
sonya_fox2 <- "https://www.foxnews.com/sports/yankees-marcus-stroman-speaks-out-following-fatal-shooting-sonya-massey-sad-society-were-living-in" 
sonya_fox2_page <- read_html(sonya_fox2) 
text_data_fox2 <- html_text(html_nodes(sonya_fox2_page, "p")) # Extract all text within the <p> tags
print(text_data_fox2)
```

#### Generate Corpus Fox 2 Article

```{r, message=F, warning=F}
corpus_sonyafox2 <- corpus(text_data_fox2)
corpus_sonyafox2 <- # REMOVING LAST THREE TEXTS DUE TO CONTENT
  corpus_subset(corpus_sonyafox2, 
                !(docnames(corpus_sonyafox2) %in% c(paste("text",1:2,sep=""),paste("text",19:28,sep=""))))
tail(corpus_sonyafox2)
head(corpus_sonyafox2)
print(corpus_sonyafox2)
summary(corpus_sonyafox2)
head(docvars(corpus_sonyafox2))
```

#### Create Tokens

```{r, message=F, warning=F}
sonya_token_fox2 <- tokens(corpus_sonyafox2) #tokenized the pdf text
sonya_nonpunc_fox2 <- tokens(text_data_fox2, remove_punct = TRUE) #remove punctuation
print(sonya_nonpunc_fox2)
```

#### Remove Punctuation, Articles, and Custom Words

```{r, message=F, warning=F}
sonya_nostop_fox2 <- tokens_select(sonya_nonpunc_fox2, pattern = stopwords("en"), selection = "remove") #remove articles
print(sonya_nostop_fox2)
fox_custom_stop_words <- c("said", "says","fox","sports","new","york")
sonya_custom_fox2 <- tokens_select(sonya_nonpunc_fox2, pattern = c((stopwords("en")),(fox_custom_stop_words)), selection = "remove") #remove custom words
print(sonya_custom_fox2)
```

#### Dataframe

```{r, message=F, warning=F}
sonya.dfm.fox2 <- dfm(sonya_nostop_fox2) #put the text into a dataframe matrix
sonya.fox2.frequency <- topfeatures(sonya.dfm.fox2, n = 10) #asked for the top 10 words in article
print(sonya.fox2.frequency)
sonya.fox2.dfm.custom <- dfm(sonya_custom_fox2)
sonya.fox2.frequency.custom <- topfeatures(sonya.fox2.dfm.custom, n = 10)
print(sonya.fox2.frequency.custom)
```

#### Table

```{r, message=F, warning=F}
dataframe.fox2 <- data.frame(sonya.fox2.frequency.custom)
colnames(dataframe.fox2) <- "frequency"
#view(dataframe.fox2.comments)
dataframe.fox2.organized <- tibble::rownames_to_column(dataframe.fox2, var = "topwords")
#view(dataframe.fox2.organized)
dataframe.fox2.categories <- dataframe.fox2.organized %>%
  mutate(Group = case_when(
    topwords %in% c("massey", "water", "pot", "black","woman","unarmed","boiling","boil","sonya") ~ "victim",  # For A, B, C -> "first"
    topwords %in% c("grayson", "officer", "deputy","police","deputy","enforcement","officers","deputies","cop","cops","violated","policy","sheriff","sheriff's") ~ "officer",   # For X, Y, Z -> "last"
    TRUE ~ "other"                            # Default case
  )) 
#view(dataframe.fox2.categories)
filter.fox2.categories <- filter(dataframe.fox2.categories) #filter variables
#filter.fox2.categories 
#write.csv(filter.fox2.categories, file = "fox2article_dataset.csv")
#View(filter.fox2.categories)
```

#### Semantic Analysis

```{r, message=F, warning=F}
fox_custom_key_words <- c("sonya","massey","grayson","officer","kill*","justified*","just*","fault*","cop*","officer*","violation*","policy","she","her","deserve*")
sonya_key_custom_fox2 <- tokens_select(sonya_nostop_fox2, pattern = fox_custom_key_words) #choose keywords to analyze
sonya.dfm.fox2.key <- dfm(sonya_key_custom_fox2)
sonya.fox2.lsa <- textmodel_lsa(sonya.fox2.dfm.custom)
sonya.fox2.lsa$matrix_low_rank
sonya.fox2.lsa$features
dataframe.fox2.lsa.features <- data.frame(sonya.fox2.lsa$features)
view(dataframe.fox2.lsa.features)
barplot(sonya.fox2.lsa$features)
predict.fox2 <- predict(sonya.fox2.lsa)
predict.fox2
textplot_network(sonya.dfm.fox2.key)
```

#### Choose Keywords to Analyze

```{r, message=F, warning=F}
kw_water_fox2 <- kwic(sonya_token_fox2, pattern =  "water") #searching for the word in association with other words
print(kw_water_fox2)
kw_rebuke_fox2 <- kwic(sonya_token_fox2, pattern =  "rebuke")
print(kw_rebuke_fox2)
kw_justification_fox2 <- kwic(sonya_token_fox2, pattern = phrase("boiling water*")) #phrase meaning
print(kw_justification_fox2)
sonya_nostop_fox2 <- tokens_select(sonya_nonpunc_fox2, pattern = stopwords("en"), selection = "remove") #remove articles
print(sonya_nostop_fox2)
sonya_threat_fox2 <- tokens_select(sonya_nostop_fox2, pattern = c("speak*","deputy*"), padding = TRUE, window = 5) #view words associated with key words
print(sonya_threat_fox2)
```

### Fox 2 Comments (n=78)

#### Import Fox 2 Comments Data

```{r, message=F,warning=F}
sonya_fox2_comments <- readtext("C:/Users/kadej/Dropbox/sonya.quanteda/fox2comments.txt") 
str(sonya_fox2_comments)
sonya_fox2_comments_raw <- sonya_fox2_comments$text
str(sonya_fox2_comments_raw)
chunks <- strsplit(sonya_fox2_comments_raw, "\nReply")[[1]]
fox2_chunks <- trimws(chunks)
fox2_chunks
fox2_cleaned_chunks <- str_extract(fox2_chunks, "[^\n]+$")
fox2_cleaned_chunks
library(stringr)
```

#### Generate Corpus Fox 2 Comments

```{r, message=F, warning=F}
corpus_sonya_fox2_comments <- corpus(fox2_cleaned_chunks) #make corpus
print(corpus_sonya_fox2_comments)
summary(corpus_sonya_fox2_comments)
head(corpus_sonya_fox2_comments)
```

#### Create Tokens, Remove Punctuation, Numbers, and Comments

```{r, message=F, warning=F}
sonya_token_fox2_comments <- tokens(corpus_sonya_fox2_comments) #tokenized the pdf text
sonya_nonpunc_fox2_comments <- tokens(corpus_sonya_fox2_comments, remove_punct = TRUE, remove_numbers = TRUE) #remove punctuation and numbers
print(sonya_nonpunc_fox2_comments)
fox_remove <- c("See more","Reply")
sonya_nostop_fox2_comments <- tokens_select(sonya_nonpunc_fox2_comments, pattern = c(stopwords("en"), fox_remove), selection = "remove") #remove articles
print(sonya_nostop_fox2_comments)
```

#### Remove Custom Words

```{r, message=F, warning=F}
#fox2_comments_custom_stop_words <- c("reply","july","replies", "share","see","like","comment")
#sonya_custom_fox2_comments <- tokens_select(sonya_nonpunc_fox2_comments, pattern = #c((stopwords("en")),(fox2_comments_custom_stop_words)), selection = "remove") #remove custom words
#print(sonya_custom_fox2_comments)
```

#### Dataframe

```{r, message=F, warning=F}
sonya.dfm.fox2.comments <- dfm(sonya_nostop_fox2_comments) #put the text into a dataframe matrix
sonya.fox2.comments.frequency <- topfeatures(sonya.dfm.fox2.comments, n = 25) #asked for the top 10 words in article
print(sonya.fox2.comments.frequency)
print(sonya.fox2.comments.frequency)
```


#### Table

```{r, message=F, warning=F}
dataframe.fox2.comments <- data.frame(sonya.fox2.comments.frequency)
colnames(dataframe.fox2.comments) <- "frequency"
#view(dataframe.fox2.comments)
dataframe.fox2.comments.organized <- tibble::rownames_to_column(dataframe.fox2.comments, var = "topwords")
#view(dataframe.fox2.comments.organized)
dataframe.fox2.comments.categories <- dataframe.fox2.comments.organized %>%
  mutate(Group = case_when(
    topwords %in% c("massey", "water", "pot", "black","woman","unarmed","boiling","boil","sonya") ~ "victim",  # For A, B, C -> "first"
    topwords %in% c("grayson", "officer", "deputy","police","deputy","enforcement","officers","deputies","cop","cops","violated","policy") ~ "officer",   # For X, Y, Z -> "last"
    TRUE ~ "other"                            # Default case
  )) 
#view(dataframe.fox2.comments.categories)
filter.fox2.comments.categories <- filter(dataframe.fox2.comments.categories) #filter variables
#write.csv(filter.fox2.comments.categories, file = "fox2_comments.csv")
#filter.fox2.comments.categories 
#View(filter.fox2.comments.categories)
```


#### Semantic Analysis

```{r, message=F, warning=F}
sonya.dfm.fox2.comments <- dfm(sonya_nostop_fox2_comments)
sonya.dfm.fox2.comments
sonya.fox2.comments.lsa <- textmodel_lsa(sonya.dfm.fox2.comments)
sonya.fox2.comments.lsa$matrix_low_rank
sonya.fox2.comments.lsa$features
dataframe.fox2.comments.lsa.features <- data.frame(sonya.fox2.comments.lsa$features)
view(dataframe.fox2.comments.lsa.features)
barplot(sonya.fox2.comments.lsa$features)
network.sonya.comments.fox2 <- textplot_network(sonya.dfm.fox2.comments)
View(sonya.dfm.fox2.comments)
sonya.fox2.comments.textstat <- quanteda.textstats::textstat_keyness(sonya.dfm.fox2.comments)
textplot_keyness(sonya.fox2.comments.textstat, show_reference = TRUE, show_legend = TRUE, n=20L, min_count = 2L, margin = 0.05, color = c("gray","darkblue"), labelcolor = "gray30", labelsize = 4, font = NULL)
fox_custom_key_words <- c("sonya","massey","grayson","officer","kill*","justified*","just*","fault*","cop*","officer*","violation*","policy","evidence*","character*","value*","interpret*","she*","water","pot","boil*")
sonya_key_custom_fox2_comments <- tokens_select(sonya_nostop_fox2_comments, pattern = fox_custom_key_words) #choose keywords to analyze
sonya.dfm.fox2.comments.key <- dfm(sonya_key_custom_fox2_comments)
textplot_network(sonya.dfm.fox2.comments.key, n=40)
```


#### Choose Keywords to Analyze

```{r, message=F, warning=F}
kw_water_fox1_comments <- kwic(sonya_token_fox1_comments, pattern =  "water") #searching for the word in association with other words
print(kw_water_fox1_comments)
kw_rebuke_fox1_comments <- kwic(sonya_token_fox1_comments, pattern =  "rebuke")
print(kw_rebuke_fox1_comments)
kw_justification_fox1_comments <- kwic(sonya_token_fox1_comments, pattern = phrase("boiling water*")) #phrase meaning
print(kw_justification_fox1_comments)
sonya_nostop_fox1_comments <- tokens_select(sonya_nonpunc_fox1_comments, pattern = stopwords("en"), selection = "remove") #remove articles
print(sonya_nostop_fox1)
sonya_threat_fox1_comments <- tokens_select(sonya_nostop_fox1_comments, pattern = c("speak*","deputy*"), padding = TRUE, window = 5) #view words associated with key words
print(sonya_threat_fox1_comments)
```


## Washington Post

### Washington Post Article 1

#### Import Washington Post Article 1 data

```{r, message=F, warning=F}
sonya_wapo1 <- "C:/Users/kadej/Dropbox/Quanteda/sonyamassey_wapo1.txt"
sonya_wapo1_page <- readtext(sonya_wapo1) 
print(sonya_wapo1_page)
```

#### Generate corpus for Washington Post 1 Article 

```{r, message=F, warning=F}
corpus_sonya_wapo1 <- corpus(sonya_wapo1_page) #make corpus
print(corpus_sonya_wapo1)
summary(corpus_sonya_wapo1)
head(docvars(corpus_sonya_wapo1))
```

#### Create tokens

```{r, message=F, warning=F}
sonya_token_wapo1 <- tokens(corpus_sonya_wapo1) #tokenized the pdf text
sonya_nonpunc_wapo1 <- tokens(corpus_sonya_wapo1, remove_punct = TRUE) #remove punctuation
print(sonya_nonpunc_wapo1)
```

#### Remove Punctuation, Articles, and Custom Words

```{r, message=F, warning=F}
sonya_nostop_wapo1 <- tokens_select(sonya_nonpunc_wapo1, pattern = stopwords("en"), selection = "remove") #remove articles
print(sonya_nostop_wapo1)
wapo_custom_stop_words <- c("said", "says")
sonya_custom_wapo1 <- tokens_select(sonya_nonpunc_wapo1, pattern = c((stopwords("en")),(wapo_custom_stop_words)), selection = "remove") #remove custom words
print(sonya_custom_wapo1)
```

#### Dataframe

```{r, message=F, warning=F}
sonya.dfm.wapo1 <- dfm(sonya_nostop_wapo1) #put the text into a dataframe matrix
sonya.wapo1.frequency <- topfeatures(sonya.dfm.wapo1, n = 10) #asked for the top 10 words in article
print(sonya.wapo1.frequency)
sonya.wapo1.dfm.custom <- dfm(sonya_custom_wapo1)
sonya.wapo1.frequency.custom <- topfeatures(sonya.wapo1.dfm.custom, n = 10)
print(sonya.wapo1.frequency.custom)
```

#### Table

```{r, message=F, warning=F}
dataframe.wapo1 <- data.frame(sonya.wapo1.frequency.custom)
colnames(dataframe.wapo1) <- "frequency"
#view(dataframe.wapo1)
dataframe.wapo1.organized <- tibble::rownames_to_column(dataframe.wapo1, var = "topwords")
view(dataframe.wapo1.organized)
dataframe.wapo1.categories <- dataframe.wapo1.organized %>%
  mutate(Group = case_when(
    topwords %in% c("massey", "water", "pot", "black","woman","unarmed","boiling","boil","sonya") ~ "victim",  # For A, B, C -> "first"
    topwords %in% c("grayson", "officer", "deputy","police","deputy","enforcement","officers","deputies","cop","cops","violated","policy") ~ "officer",   # For X, Y, Z -> "last"
    TRUE ~ "other"                            # Default case
  )) 
view(dataframe.wapo1.categories)
filter.wapo1.categories <- filter(dataframe.wapo1.categories) #filter variables
filter.wapo1.categories 
View(filter.wapo1.categories)
```

#### Choose keywords to analyze

```{r, message=F, warning=F}
kw_water_wapo1 <- kwic(sonya_token_wapo1, pattern =  "water") #searching for the word in association with other words
print(kw_water_wapo1)
kw_justification_wapo1 <- kwic(sonya_token_wapo1, pattern = phrase("boiling water*")) #phrase meaning
print(kw_justification_wapo1)
sonya_nostop_wapo1 <- tokens_select(sonya_nonpunc_wapo1, pattern = stopwords("en"), selection = "remove") #remove articles
print(sonya_nostop_wapo1)
sonya_threat_wapo1 <- tokens_select(sonya_nostop_wapo1, pattern = c("speak*","deputy*"), padding = TRUE, window = 5) #view words associated with key words
print(sonya_threat_wapo1)
```


### Washington Post 1 Comments 
#### Import Data
```{r, message=F, warning=F}
sonya_wapo1_comments <- "C:/Users/kadej/Dropbox/Quanteda/wapo1comments.txt"
sonya_wapo1_page_comments <- readtext(sonya_wapo1_comments) 
print(sonya_wapo1_page_comments)
```

#### Generate Corpus

```{r, message=F, warning=F}
corpus_sonya_wapo1_comments <- corpus(sonya_wapo1_page_comments) #make corpus
print(corpus_sonya_wapo1_comments)
summary(corpus_sonya_wapo1_comments)
head(docvars(corpus_sonya_wapo1_comments))
```

#### Create Tokens, Remove Punctuation, Numbers, and Comments

```{r, message=F, warning=F}
sonya_token_wapo1_comments <- tokens(corpus_sonya_wapo1_comments) #tokenized the pdf text
sonya_nonpunc_wapo1_comments <- tokens(corpus_sonya_wapo1_comments, remove_punct = TRUE, remove_numbers = TRUE) #remove punctuation and numbers
print(sonya_nonpunc_wapo1_comments)
sonya_nostop_wapo1_comments <- tokens_select(sonya_nonpunc_wapo1_comments, pattern = stopwords("en"), selection = "remove") #remove articles
print(sonya_nostop_wapo1_comments)
```

#### Remove Custom Words

```{r, message=F, warning=F}
wapo1_comments_custom_stop_words <- c("reply","july","replies", "share","comments","recommended","expand_more","wrre","like")
sonya_custom_wapo1_comments <- tokens_select(sonya_nonpunc_wapo1_comments, pattern = c((stopwords("en")),(wapo1_comments_custom_stop_words)), selection = "remove") #remove custom words
```

#### Dataframe

```{r, message=F, warning=F}
print(sonya_custom_wapo1_comments)
#dataframe
sonya.dfm.wapo1.comments <- dfm(sonya_nostop_wapo1_comments) #put the text into a dataframe matrix
sonya.wapo1.comments.frequency <- topfeatures(sonya.dfm.wapo1.comments, n = 10) #asked for the top 10 words in article
print(sonya.wapo1.comments.frequency)
sonya.wapo1.comments.dfm.custom <- dfm(sonya_custom_wapo1_comments)
sonya.wapo1.comments.frequency.custom <- topfeatures(sonya.wapo1.comments.dfm.custom, n = 10)
print(sonya.wapo1.comments.frequency.custom)
```

#### Table

```{r, message=F, warning=F}
dataframe.wapo1.comments <- data.frame(sonya.wapo1.comments.frequency.custom)
colnames(dataframe.wapo1.comments) <- "frequency"
#view(dataframe.wapo1.comments)
dataframe.wapo1.comments.organized <- tibble::rownames_to_column(dataframe.wapo1.comments, var = "topwords")
view(dataframe.wapo1.comments.organized)
dataframe.wapo1.comments.categories <- dataframe.wapo1.comments.organized %>%
  mutate(Group = case_when(
    topwords %in% c("massey", "water", "pot", "black","woman","unarmed","boiling","boil","sonya") ~ "victim",  # For A, B, C -> "first"
    topwords %in% c("grayson", "officer", "deputy","police","deputy","enforcement","officers","deputies","cop","cops","violated","policy") ~ "officer",   # For X, Y, Z -> "last"
    TRUE ~ "other"                            # Default case
  )) 
view(dataframe.wapo1.comments.categories)
filter.wapo1.comments.categories <- filter(dataframe.wapo1.comments.categories) #filter variables
filter.wapo1.comments.categories 
View(filter.wapo1.comments.categories)
```



### Washington Post 2 Article

#### Import Washington Post 2 Article Data

```{r, message=F, warning=F}
sonya_wapo2 <- "C:/Users/kadej/Dropbox/Quanteda/sonyamassey_wapo2.txt"
sonya_wapo2_page <- readtext(sonya_wapo2) 
print(sonya_wapo2_page)
```

#### Generate Corpus for Washington Post 2 Article

```{r, message=F. warning=F}
corpus_sonya_wapo2 <- corpus(sonya_wapo2_page) #make corpus
print(corpus_sonya_wapo2)
summary(corpus_sonya_wapo2)
head(docvars(corpus_sonya_wapo2))
```

#### Create Tokens

```{r, message=F. warning=F}
sonya_token_wapo2 <- tokens(corpus_sonya_wapo2) #tokenized the pdf text
sonya_nonpunc_wapo2 <- tokens(corpus_sonya_wapo2, remove_punct = TRUE) #remove punctuation
print(sonya_nonpunc_wapo2)
```

#### Remove Punctuation, Articles, and Custom Words

```{r, message=F. warning=F}
sonya_nostop_wapo2 <- tokens_select(sonya_nonpunc_wapo2, pattern = stopwords("en"), selection = "remove") #remove articles
print(sonya_nostop_wapo2)
wapo_custom_stop_words <- c("said", "says")
sonya_custom_wapo2 <- tokens_select(sonya_nonpunc_wapo2, pattern = c((stopwords("en")),(wapo_custom_stop_words)), selection = "remove") #remove custom words
print(sonya_custom_wapo2)
```

#### Dataframe

```{r, message=F. warning=F}
sonya.dfm.wapo2 <- dfm(sonya_nostop_wapo2) #put the text into a dataframe matrix
sonya.wapo2.frequency <- topfeatures(sonya.dfm.wapo2, n = 10) #asked for the top 10 words in article
print(sonya.wapo2.frequency)
sonya.wapo2.dfm.custom <- dfm(sonya_custom_wapo2)
sonya.wapo2.frequency.custom <- topfeatures(sonya.wapo2.dfm.custom, n = 10)
print(sonya.wapo2.frequency.custom)
```

#### Table

```{r, message=F, warning=F}
dataframe.wapo2 <- data.frame(sonya.wapo2.frequency.custom)
colnames(dataframe.wapo2) <- "frequency"
#view(dataframe.wapo2.comments)
dataframe.wapo2.organized <- tibble::rownames_to_column(dataframe.wapo2, var = "topwords")
view(dataframe.wapo2.organized)
dataframe.wapo2.categories <- dataframe.wapo2.organized %>%
  mutate(Group = case_when(
    topwords %in% c("massey", "water", "pot", "black","woman","unarmed","boiling","boil","sonya") ~ "victim",  # For A, B, C -> "first"
    topwords %in% c("grayson", "officer", "deputy","police","deputy","enforcement","officers","deputies","cop","cops","violated","policy") ~ "officer",   # For X, Y, Z -> "last"
    TRUE ~ "other"                            # Default case
  )) 
view(dataframe.wapo2.categories)
filter.wapo2.categories <- filter(dataframe.wapo2.categories) #filter variables
filter.wapo2.categories 
View(filter.wapo2.categories)
```

#### Choose Keywords

```{r, message=F. warning=F}
kw_water_wapo2 <- kwic(sonya_token_wapo2, pattern =  "water") #searching for the word in association with other words
print(kw_water_wapo2)
kw_justification_wapo2 <- kwic(sonya_token_wapo2, pattern = phrase("boiling water*")) #phrase meaning
print(kw_justification_wapo2)
sonya_nostop_wapo2 <- tokens_select(sonya_nonpunc_wapo2, pattern = stopwords("en"), selection = "remove") #remove articles
print(sonya_nostop_wapo2)
sonya_threat_wapo2 <- tokens_select(sonya_nostop_wapo2, pattern = c("speak*","deputy*"), padding = TRUE, window = 5) #view words associated with key words
print(sonya_threat_wapo2)
```



### washington Post 2 Comments

#### Import Washington Post 2 Comment Data

```{r, message=F, warning=F}
sonya_wapo2_comments <- "C:/Users/kadej/Dropbox/Quanteda/wapo2comments.txt"
sonya_wapo2_page_comments <- readtext(sonya_wapo2_comments) 
print(sonya_wapo2_page_comments)
```

#### Generate Corpus for Washington Post 2 Comments

```{r, message=F, warning=F}
corpus_sonya_wapo2_comments <- corpus(sonya_wapo2_page_comments) #make corpus
print(corpus_sonya_wapo2_comments)
summary(corpus_sonya_wapo2_comments)
head(docvars(corpus_sonya_wapo2_comments))
```

#### Create Tokens, Remove Punctuation, Numbers, and Comments

```{r, message=F, warning=F}
sonya_token_wapo2_comments <- tokens(corpus_sonya_wapo2_comments) #tokenized the pdf text
sonya_nonpunc_wapo2_comments <- tokens(corpus_sonya_wapo2_comments, remove_punct = TRUE, remove_numbers = TRUE) #remove punctuation and numbers
print(sonya_nonpunc_wapo2_comments)
sonya_nostop_wapo2_comments <- tokens_select(sonya_nonpunc_wapo2_comments, pattern = stopwords("en"), selection = "remove") #remove articles
print(sonya_nostop_wapo2_comments)
```

#### Remove Custom Stop Words

```{r, message=F, warning=F}
wapo2_comments_custom_stop_words <- c("reply","july","replies", "share","comments","recommended","expand_more","wrre")
sonya_custom_wapo2_comments <- tokens_select(sonya_nonpunc_wapo2_comments, pattern = c((stopwords("en")),(wapo2_comments_custom_stop_words)), selection = "remove") #remove custom words
print(sonya_custom_wapo2_comments)
```

#### Dataframe

```{r, message=F, warning=F}
#dataframe
sonya.dfm.wapo2.comments <- dfm(sonya_nostop_wapo2_comments) #put the text into a dataframe matrix
sonya.wapo2.comments.frequency <- topfeatures(sonya.dfm.wapo2.comments, n = 10) #asked for the top 10 words in article
print(sonya.wapo2.comments.frequency)
sonya.wapo2.comments.dfm.custom <- dfm(sonya_custom_wapo2_comments)
sonya.wapo2.comments.frequency.custom <- topfeatures(sonya.wapo2.comments.dfm.custom, n = 10)
print(sonya.wapo2.comments.frequency.custom)
```

#### Table

```{r, message=F, warning=F}
dataframe.wapo2.comments <- data.frame(sonya.wapo2.comments.frequency.custom)
colnames(dataframe.wapo2.comments) <- "frequency"
#view(dataframe.wapo2.comments)
dataframe.wapo2.comments.organized <- tibble::rownames_to_column(dataframe.wapo2.comments, var = "topwords")
view(dataframe.wapo2.comments.organized)
dataframe.wapo2.comments.categories <- dataframe.wapo2.comments.organized %>%
  mutate(Group = case_when(
    topwords %in% c("massey", "water", "pot", "black","woman","unarmed","boiling","boil","sonya") ~ "victim",  # For A, B, C -> "first"
    topwords %in% c("grayson", "officer", "deputy","police","deputy","enforcement","officers","deputies","cop","cops","violated","policy") ~ "officer",   # For X, Y, Z -> "last"
    TRUE ~ "other"                            # Default case
  )) 
view(dataframe.wapo2.comments.categories)
filter.wapo2.comments.categories <- filter(dataframe.wapo2.comments.categories) #filter variables
filter.wapo2.comments.categories 
View(filter.wapo2.comments.categories)
```



## ABC

### ABC Article 1

#### Import ABC Article 1 data

```{r, message=F, warning=F}
sonya_abc1 <- "https://abcnews.go.com/US/illinois-woman-dies-after-shot-deputy-involved-incident/story?id=111880175" 
sonya_abc1_page <- read_html(sonya_abc1) 
text_data_abc1 <- html_text(html_nodes(sonya_abc1_page, "p")) # Extract all text within the <p> tags
print(text_data_abc1)
```

#### Generate corpus for ABC 1 Article 

```{r, message=F, warning=F}
corpus_sonyaabc1 <- corpus(text_data_abc1)
corpus_sonyaabc1 <- # REMOVING LAST THREE TEXTS DUE TO CONTENT
  corpus_subset(corpus_sonyaabc1, 
                !(docnames(corpus_sonyaabc1) %in% c("text16")))
tail(corpus_sonyaabc1)
head(corpus_sonyaabc1)
print(corpus_sonyaabc1)
summary(corpus_sonyaabc1)
head(docvars(corpus_sonyaabc1))
```

#### Generate tokens for ABC 1 Article

```{r, message=F, warning=F}
sonya_token_abc1 <- tokens(corpus_sonyaabc1) #tokenized the pdf text
sonya_nonpunc_abc1 <- tokens(text_data_abc1, remove_punct = TRUE) #remove punctuation
print(sonya_nonpunc_abc1)
```

#### Remove Punctuation, Articles, and Custom Words

```{r, message=F, warning=F}
sonya_nostop_abc1 <- tokens_select(sonya_nonpunc_abc1, pattern = stopwords("en"), selection = "remove") #remove articles
print(sonya_nostop_abc1)
abc_custom_stop_words <- c("statement", "news","abc","sangamon","county","state","illinois")
sonya_custom_abc1 <- tokens_select(sonya_nonpunc_abc1, pattern = c((stopwords("en")),(abc_custom_stop_words)), selection = "remove") #remove custom words
print(sonya_custom_abc1)
```

#### Dataframe

```{r, message=F, warning=F}
sonya.dfm.abc1 <- dfm(sonya_nostop_abc1) #put the text into a dataframe matrix
sonya.abc1.frequency <- topfeatures(sonya.dfm.abc1, n = 10) #asked for the top 10 words in article
print(sonya.abc1.frequency)
sonya.abc1.dfm.custom <- dfm(sonya_custom_abc1)
sonya.abc1.frequency.custom <- topfeatures(sonya.abc1.dfm.custom, n = 10)
print(sonya.abc1.frequency.custom)
```

#### Table

```{r, message=F, warning=F}
dataframe.abc1 <- data.frame(sonya.abc1.frequency.custom)
colnames(dataframe.abc1) <- "frequency"
#view(dataframe.wapo1)
dataframe.abc1.organized <- tibble::rownames_to_column(dataframe.abc1, var = "topwords")
#view(dataframe.abc1.organized)
dataframe.abc1.categories <- dataframe.abc1.organized %>%
  mutate(Group = case_when(
    topwords %in% c("massey", "water", "pot", "black","woman","unarmed","boiling","boil","sonya") ~ "victim",  # For A, B, C -> "first"
    topwords %in% c("grayson", "officer", "deputy","police","deputy","enforcement","officers","deputies","cop","cops","violated","policy","sheriff","sheriff's") ~ "officer",   # For X, Y, Z -> "last"
    TRUE ~ "other"                            # Default case
  )) 
#view(dataframe.abc1.categories)
filter.abc1.categories <- filter(dataframe.abc1.categories) #filter variables
#filter.abc1.categories 
#View(filter.abc1.categories)
```

#### Choose keywords to analyze

```{r, message=F, warning=F}
kw_water_abc1 <- kwic(sonya_token_abc1, pattern =  "water") #searching for the word in association with other words
print(kw_water_abc1)
kw_rebuke_abc1 <- kwic(sonya_token_abc1, pattern =  "rebuke")
print(kw_rebuke_abc1)
kw_justification_abc1 <- kwic(sonya_token_abc1, pattern = phrase("boiling water*")) #phrase meaning
print(kw_justification_abc1)
sonya_nostop_abc1 <- tokens_select(sonya_nonpunc_abc1, pattern = stopwords("en"), selection = "remove") #remove articles
print(sonya_nostop_abc1)
sonya_threat_abc1 <- tokens_select(sonya_nostop_abc1, pattern = c("said*","officer*"), padding = TRUE, window = 5) #view words associated with key words
print(sonya_threat_abc1)
```

###ABC Article 2

#### Import ABC Article 2 data

```{r, message=F, warning=F}
sonya_abc2 <- "https://abcnews.go.com/US/deputy-fatally-shot-sonya-massey-discharged-army-misconduct/story?id=112264355" 
sonya_abc2_page <- read_html(sonya_abc2) 
text_data_abc2 <- html_text(html_nodes(sonya_abc2_page, "p")) # Extract all text within the <p> tags
print(text_data_abc2)
```

#### Generate ABC Article 2 Corpus

```{r, message=F, warning=F}
corpus_sonyaabc2 <- corpus(text_data_abc2)
corpus_sonyaabc2 <- # REMOVING LAST THREE TEXTS DUE TO CONTENT
  corpus_subset(corpus_sonyaabc2, 
                !(docnames(corpus_sonyaabc2) %in% c("text28")))
tail(corpus_sonyaabc2)
head(corpus_sonyaabc2)
print(corpus_sonyaabc2)
summary(corpus_sonyaabc2)
head(docvars(corpus_sonyaabc2))
```

#### Create Tokens

```{r, message=F, warning=F}
sonya_token_abc2 <- tokens(corpus_sonyaabc2) #tokenized the pdf text
sonya_nonpunc_abc2 <- tokens(text_data_abc2, remove_punct = TRUE) #remove punctuation
print(sonya_nonpunc_abc2)
```

#### Remove Punctuation, Articles, and Custom Words

```{r, message=F, warning=F}
sonya_nostop_abc2 <- tokens_select(sonya_nonpunc_abc2, pattern = stopwords("en"), selection = "remove") #remove articles
print(sonya_nostop_abc2)
abc_custom_stop_words <- c("statement", "news","abc","sangamon","county","state","illinois","may","2021","also")
sonya_custom_abc2 <- tokens_select(sonya_nonpunc_abc2, pattern = c((stopwords("en")),(abc_custom_stop_words)), selection = "remove") #remove custom words
print(sonya_custom_abc2)
```

#### Dataframe

```{r, message=F, warning=F}
#dataframe
sonya.dfm.abc2 <- dfm(sonya_nostop_abc2) #put the text into a dataframe matrix
sonya.abc2.frequency <- topfeatures(sonya.dfm.abc2, n = 10) #asked for the top 10 words in article
print(sonya.abc2.frequency)
sonya.abc2.dfm.custom <- dfm(sonya_custom_abc2)
sonya.abc2.frequency.custom <- topfeatures(sonya.abc2.dfm.custom, n = 10)
print(sonya.abc2.frequency.custom)
```

#### Table

```{r, message=F, warning=F}
dataframe.abc2 <- data.frame(sonya.abc2.frequency.custom)
colnames(dataframe.abc2) <- "frequency"
#view(dataframe.wapo1)
dataframe.abc2.organized <- tibble::rownames_to_column(dataframe.abc2, var = "topwords")
#view(dataframe.abc2.organized)
dataframe.abc2.categories <- dataframe.abc2.organized %>%
  mutate(Group = case_when(
    topwords %in% c("massey", "water", "pot", "black","woman","unarmed","boiling","boil","sonya") ~ "victim",  # For A, B, C -> "first"
    topwords %in% c("grayson", "officer", "deputy","police","deputy","enforcement","officers","deputies","cop","cops","violated","policy","sheriff","sheriff's") ~ "officer",   # For X, Y, Z -> "last"
    TRUE ~ "other"                            # Default case
  )) 
#view(dataframe.abc2.categories)
filter.abc2.categories <- filter(dataframe.abc2.categories) #filter variables
#filter.abc2.categories 
#View(filter.abc2.categories)
```

#### Keywords

```{r, message=F, warning=F}
kw_water_abc2 <- kwic(sonya_token_abc2, pattern =  "water") #searching for the word in association with other words
print(kw_water_abc2)
kw_rebuke_abc2 <- kwic(sonya_token_abc2, pattern =  "rebuke")
print(kw_rebuke_abc2)
kw_justification_abc2 <- kwic(sonya_token_abc2, pattern = phrase("boiling water*")) #phrase meaning
print(kw_justification_abc2)
sonya_nostop_abc2 <- tokens_select(sonya_nonpunc_abc2, pattern = stopwords("en"), selection = "remove") #remove articles
print(sonya_nostop_abc2)
sonya_threat_abc2 <- tokens_select(sonya_nostop_abc2, pattern = c("said*","officer*"), padding = TRUE, window = 5) #view words associated with key words
print(sonya_threat_abc2)
```

## NBC

### NBC Article 1

#### Import NBC Article 1 data

```{r, message=F, warning=F}
sonya_nbc1 <- "https://www.nbcnews.com/news/us-news/illinois-woman-called-police-possible-intruder-killed-deputies-attorne-rcna161673"
sonya_nbc1_page <- read_html(sonya_nbc1) 
text_data_nbc1 <- html_text(html_nodes(sonya_nbc1_page, "p")) # Extract all text within the <p> tags
print(text_data_nbc1)
```

#### Generate corpus for NBC 1 Article 

```{r, message=F, warning=F}
corpus_sonyanbc1 <- corpus(text_data_nbc1)
corpus_sonyanbc1 <- # REMOVING LAST THREE TEXTS DUE TO CONTENT
  corpus_subset(corpus_sonyanbc1, 
                !(docnames(corpus_sonyanbc1) %in% c(paste("text",1:10,sep=""),paste("text",26:27,sep=""))))
tail(corpus_sonyanbc1)
head(corpus_sonyanbc1)
print(corpus_sonyanbc1)
summary(corpus_sonyanbc1)
head(docvars(corpus_sonyanbc1))
```

#### Create tokens

```{r, message=F, warning=F}
sonya_token_nbc1 <- tokens(corpus_sonyanbc1) #tokenized the pdf text
sonya_nonpunc_nbc1 <- tokens(text_data_nbc1, remove_punct = TRUE) #remove punctuation
print(sonya_nonpunc_nbc1)
```

#### Remove Punctuation, Articles, and Custom Words

```{r, message=F, warning=F}
sonya_nostop_nbc1 <- tokens_select(sonya_nonpunc_nbc1, pattern = stopwords("en"), selection = "remove") #remove articles
print(sonya_nostop_nbc1)
nbc_custom_stop_words <- c("said")
sonya_custom_nbc1 <- tokens_select(sonya_nonpunc_nbc1, pattern = c((stopwords("en")),(nbc_custom_stop_words)), selection = "remove") #remove custom words
print(sonya_custom_nbc1)
```

#### Dataframe

```{r, message=F, warning=F}
sonya.dfm.nbc1 <- dfm(sonya_nostop_nbc1) #put the text into a dataframe matrix
sonya.nbc1.frequency <- topfeatures(sonya.dfm.nbc1, n = 10) #asked for the top 10 words in article
print(sonya.nbc1.frequency)
sonya.nbc1.dfm.custom <- dfm(sonya_custom_nbc1)
sonya.nbc1.frequency.custom <- topfeatures(sonya.nbc1.dfm.custom, n = 10)
print(sonya.nbc1.frequency.custom)
```

#### Table

```{r, message=F, warning=F}
dataframe.nbc1 <- data.frame(sonya.nbc1.frequency.custom)
colnames(dataframe.nbc1) <- "frequency"
#view(dataframe.nbc1)
dataframe.nbc1.organized <- tibble::rownames_to_column(dataframe.nbc1, var = "topwords")
#view(dataframe.nbc1.organized)
dataframe.nbc1.categories <- dataframe.nbc1.organized %>%
  mutate(Group = case_when(
    topwords %in% c("massey", "water", "pot", "black","woman","unarmed","boiling","boil","sonya") ~ "victim",  # For A, B, C -> "first"
    topwords %in% c("grayson", "officer", "deputy","police","deputy","enforcement","officers","deputies","cop","cops","violated","policy") ~ "officer",   # For X, Y, Z -> "last"
    TRUE ~ "other"                            # Default case
  )) 
#view(dataframe.nbc1.categories)
filter.nbc1.categories <- filter(dataframe.nbc1.categories) #filter variables
#filter.nbc1.categories 
#View(filter.nbc1.categories)
```

#### Choose keywords to analyze

```{r, message=F, warning=F}
kw_water_nbc1 <- kwic(sonya_token_nbc1, pattern =  "just*") #searching for the word in association with other words
print(kw_water_nbc1)
kw_rebuke_nbc1 <- kwic(sonya_token_nbc1, pattern =  "Grayson")
print(kw_rebuke_nbc1)
kw_justification_nbc1 <- kwic(sonya_token_nbc1, pattern = phrase("kill*")) #phrase meaning
print(kw_justification_nbc1)
sonya_nostop_nbc1 <- tokens_select(sonya_nonpunc_nbc1, pattern = stopwords("en"), selection = "remove") #remove articles
print(sonya_nostop_nbc1)
sonya_threat_nbc1 <- tokens_select(sonya_nostop_nbc1, pattern = c("threat*"), padding = TRUE, window = 5) #view words associated with key words
print(sonya_threat_nbc1)
```

### NBC Article 2

#### Import NBC Article 2 Data

```{r, message=F, warning=F}
sonya_nbc2 <- "https://www.nbcnews.com/news/us-news/charges-filed-illinois-deputy-death-sonya-massey-rcna162456"
sonya_nbc2_page <- read_html(sonya_nbc2) 
text_data_nbc2 <- html_text(html_nodes(sonya_nbc2_page, "p")) # Extract all text within the <p> tags
print(text_data_nbc2)
```

#### Generate corpus for NBC 2 Article

```{r, message=F, warning=F}
corpus_sonyanbc2 <- corpus(text_data_nbc2)
corpus_sonyanbc2 <- # REMOVING LAST THREE TEXTS DUE TO CONTENT
  corpus_subset(corpus_sonyanbc2, 
                !(docnames(corpus_sonyanbc2) %in% c(paste("text",1:11,sep=""),paste("text",36:38,sep=""))))
tail(corpus_sonyanbc2)
head(corpus_sonyanbc2)
print(corpus_sonyanbc2)
summary(corpus_sonyanbc2)
head(docvars(corpus_sonyanbc2))
```

#### Create Tokens

```{r, message=F, warning=F}
sonya_token_nbc2 <- tokens(corpus_sonyanbc2) #tokenized the pdf text
sonya_nonpunc_nbc2 <- tokens(text_data_nbc2, remove_punct = TRUE) #remove punctuation
print(sonya_nonpunc_nbc2)
```

#### Remove Punctuation, Articles, and Custom Words

```{r, message=F, warning=F}
sonya_nostop_nbc2 <- tokens_select(sonya_nonpunc_nbc2, pattern = stopwords("en"), selection = "remove") #remove articles
print(sonya_nostop_nbc2)
nbc_custom_stop_words <- c("said")
sonya_custom_nbc2 <- tokens_select(sonya_nonpunc_nbc2, pattern = c((stopwords("en")),(nbc_custom_stop_words)), selection = "remove") #remove custom words
print(sonya_custom_nbc2)
```

#### Dataframe

```{r, message=F, warning=F}
sonya.dfm.nbc2 <- dfm(sonya_nostop_nbc2) #put the text into a dataframe matrix
sonya.nbc2.frequency <- topfeatures(sonya.dfm.nbc2, n = 10) #asked for the top 10 words in article
print(sonya.nbc2.frequency)
sonya.nbc2.dfm.custom <- dfm(sonya_custom_nbc2)
sonya.nbc2.frequency.custom <- topfeatures(sonya.nbc2.dfm.custom, n = 10)
print(sonya.nbc2.frequency.custom)
```

#### Table

```{r, message=F, warning=F}
dataframe.nbc2 <- data.frame(sonya.nbc2.frequency.custom)
colnames(dataframe.nbc2) <- "frequency"
#view(dataframe.nbc2)
dataframe.nbc2.organized <- tibble::rownames_to_column(dataframe.nbc2, var = "topwords")
#view(dataframe.nbc2.organized)
dataframe.nbc2.categories <- dataframe.nbc2.organized %>%
  mutate(Group = case_when(
    topwords %in% c("massey", "water", "pot", "black","woman","unarmed","boiling","boil","sonya") ~ "victim",  # For A, B, C -> "first"
    topwords %in% c("grayson", "officer", "deputy","police","deputy","enforcement","officers","deputies","cop","cops","violated","policy") ~ "officer",   # For X, Y, Z -> "last"
    TRUE ~ "other"                            # Default case
  )) 
#view(dataframe.nbc2.categories)
filter.nbc2.categories <- filter(dataframe.nbc2.categories) #filter variables
#filter.nbc2.categories 
#View(filter.nbc2.categories)
```

#### Choose Keywords

```{r, message=F, warning=F}
kw_water_nbc2 <- kwic(sonya_token_nbc2, pattern =  "just*") #searching for the word in association with other words
print(kw_water_nbc2)
kw_rebuke_nbc2 <- kwic(sonya_token_nbc2, pattern =  "Grayson")
print(kw_rebuke_nbc2)
kw_justification_nbc2 <- kwic(sonya_token_nbc2, pattern = phrase("kill*")) #phrase meaning
print(kw_justification_nbc2)
sonya_nostop_nbc2 <- tokens_select(sonya_nonpunc_nbc2, pattern = stopwords("en"), selection = "remove") #remove articles
print(sonya_nostop_nbc2)
sonya_threat_nbc2 <- tokens_select(sonya_nostop_nbc2, pattern = c("threat*"), padding = TRUE, window = 5) #view words associated with key words
print(sonya_threat_nbc2)
```

## New York Times

### New York Times Article 1

#### Import New York Times Article 1 data

```{r, message=F, warning=F}
sonya_nytimes1 <- "C:/Users/kadej/Dropbox/Quanteda/sonyamassey_nytimes.txt"
sonya_nytimes1_page <- readtext(sonya_nytimes1) 
print(sonya_nytimes1_page)
```

#### Generate corpus for New York Times 1 Article 

```{r, message=F, warning=F}
corpus_sonyanytimes1 <- corpus(sonya_nytimes1_page) #make corpus
print(corpus_sonyanytimes1)
summary(corpus_sonyanytimes1)
head(docvars(corpus_sonyanytimes1))
```

#### Generate tokens

```{r, message=F, warning=F}
sonya_token_nytimes1 <- tokens(corpus_sonyanytimes1) #tokenized the pdf text
sonya_nonpunc_nytimes1 <- tokens(corpus_sonyanytimes1, remove_punct = TRUE) #remove punctuation
print(sonya_nonpunc_nytimes1)
```

#### Remove Punctuation, Articles, and Custom Words

```{r, message=F, warning=F}
sonya_nostop_nytimes1 <- tokens_select(sonya_nonpunc_nytimes1, pattern = stopwords("en"), selection = "remove") #remove articles
print(sonya_nostop_nytimes1)
nytimes_custom_stop_words <- c("said","mr","ms")
sonya_custom_nytimes1 <- tokens_select(sonya_nonpunc_nytimes1, pattern = c((stopwords("en")),(nytimes_custom_stop_words)), selection = "remove") #remove custom words
print(sonya_custom_nytimes1)
```

#### Dataframe

```{r, message=F, warning=F}
sonya.dfm.nytimes1 <- dfm(sonya_nostop_nytimes1) #put the text into a dataframe matrix
sonya.nytimes1.frequency <- topfeatures(sonya.dfm.nytimes1, n = 10) #asked for the top 10 words in article
print(sonya.nytimes1.frequency)
sonya.nytimes1.dfm.custom <- dfm(sonya_custom_nytimes1)
sonya.nytimes1.frequency.custom <- topfeatures(sonya.nytimes1.dfm.custom, n = 10)
print(sonya.nytimes1.frequency.custom)
```

#### Table

```{r, message=F, warning=F}
dataframe.nytimes1 <- data.frame(sonya.nytimes1.frequency.custom)
colnames(dataframe.nytimes1) <- "frequency"
#view(dataframe.nytimes1)
dataframe.nytimes1.organized <- tibble::rownames_to_column(dataframe.nytimes1, var = "topwords")
#view(dataframe.nytimes1.organized)
dataframe.nytimes1.categories <- dataframe.nytimes1.organized %>%
  mutate(Group = case_when(
    topwords %in% c("massey", "water", "pot", "black","woman","unarmed","boiling","boil","sonya") ~ "victim",  # For A, B, C -> "first"
    topwords %in% c("grayson", "officer", "deputy","police","deputy","enforcement","officers","deputies","cop","cops","violated","policy") ~ "officer",   # For X, Y, Z -> "last"
    TRUE ~ "other"                            # Default case
  )) 
#view(dataframe.nytimes1.categories)
filter.nytimes1.categories <- filter(dataframe.nytimes1.categories) #filter variables
#filter.nytimes1.categories 
#View(filter.nytimes1.categories)
```


#### Choose keywords to analyze

```{r, message=F, warning=F}
kw_water_nytimes1 <- kwic(sonya_token_nytimes1, pattern =  "water") #searching for the word in association with other words
print(kw_water_nytimes1)
kw_justification_nytimes1 <- kwic(sonya_token_nytimes1, pattern = phrase("boiling water*")) #phrase meaning
print(kw_justification_nytimes1)
sonya_nostop_nytimes1 <- tokens_select(sonya_nonpunc_nytimes1, pattern = stopwords("en"), selection = "remove") #remove articles
print(sonya_nostop_nytimes1)
sonya_threat_nytimes1 <- tokens_select(sonya_nostop_nytimes1, pattern = c("speak*","deputy*"), padding = TRUE, window = 5) #view words associated with key words
print(sonya_threat_nytimes1)
```

### New York Times 2 Article

#### Import New York Times 2 Article Data

```{r, message=F, warning=F}
sonya_nytimes2 <- "C:/Users/kadej/Dropbox/Quanteda/sonyamassey_nytimes2.txt"
sonya_nytimes2_page <- readtext(sonya_nytimes2) 
print(sonya_nytimes2_page)
```

#### Generate Corpus for New York Times 2 Article

```{r, message=F, warning=F}
corpus_sonyanytimes2 <- corpus(sonya_nytimes2_page) #make corpus
print(corpus_sonyanytimes2)
summary(corpus_sonyanytimes2)
head(docvars(corpus_sonyanytimes2))
```

#### Create Tokens

```{r, message=F, warning=F}
sonya_token_nytimes2 <- tokens(corpus_sonyanytimes2) #tokenized the pdf text
sonya_nonpunc_nytimes2 <- tokens(corpus_sonyanytimes2, remove_punct = TRUE) #remove punctuation
print(sonya_nonpunc_nytimes2)
```

#### Remove Punctuation, Articles, and Custom Words

```{r, message=F, warning=F}
sonya_nostop_nytimes2 <- tokens_select(sonya_nonpunc_nytimes2, pattern = stopwords("en"), selection = "remove") #remove articles
print(sonya_nostop_nytimes2)
nytimes_custom_stop_words <- c("said","mr","ms")
sonya_custom_nytimes2 <- tokens_select(sonya_nonpunc_nytimes2, pattern = c((stopwords("en")),(nytimes_custom_stop_words)), selection = "remove") #remove custom words
print(sonya_custom_nytimes2)
```

#### Dataframe

```{r, message=F, warning=F}
sonya.dfm.nytimes2 <- dfm(sonya_nostop_nytimes2) #put the text into a dataframe matrix
sonya.nytimes2.frequency <- topfeatures(sonya.dfm.nytimes2, n = 10) #asked for the top 10 words in article
print(sonya.nytimes2.frequency)
sonya.nytimes2.dfm.custom <- dfm(sonya_custom_nytimes2)
sonya.nytimes2.frequency.custom <- topfeatures(sonya.nytimes2.dfm.custom, n = 10)
print(sonya.nytimes2.frequency.custom)
```

#### Table
```{r, message=F, warning=F}
dataframe.nytimes2 <- data.frame(sonya.nytimes2.frequency.custom)
colnames(dataframe.nytimes2) <- "frequency"
#view(dataframe.nytimes1)
dataframe.nytimes2.organized <- tibble::rownames_to_column(dataframe.nytimes2, var = "topwords")
#view(dataframe.nytimes2.organized)
dataframe.nytimes2.categories <- dataframe.nytimes2.organized %>%
  mutate(Group = case_when(
    topwords %in% c("massey", "water", "pot", "black","woman","unarmed","boiling","boil","sonya") ~ "victim",  # For A, B, C -> "first"
    topwords %in% c("grayson", "officer", "deputy","police","deputy","enforcement","officers","deputies","cop","cops","violated","policy") ~ "officer",   # For X, Y, Z -> "last"
    TRUE ~ "other"                            # Default case
  )) 
#view(dataframe.nytimes2.categories)
filter.nytimes2.categories <- filter(dataframe.nytimes2.categories) #filter variables
#filter.nytimes2.categories 
#View(filter.nytimes2.categories)
```

#### Keywords

```{r, message=F, warning=F}
kw_water_nytimes2 <- kwic(sonya_token_nytimes2, pattern =  "water") #searching for the word in association with other words
print(kw_water_nytimes2)
kw_justification_nytimes2 <- kwic(sonya_token_nytimes2, pattern = phrase("boiling water*")) #phrase meaning
print(kw_justification_nytimes2)
sonya_nostop_nytimes2 <- tokens_select(sonya_nonpunc_nytimes2, pattern = stopwords("en"), selection = "remove") #remove articles
print(sonya_nostop_nytimes2)
sonya_threat_nytimes2 <- tokens_select(sonya_nostop_nytimes2, pattern = c("speak*","deputy*"), padding = TRUE, window = 5) #view words associated with key words
print(sonya_threat_nytimes2)
```

### New York Times 2 Comments

#### Import New York Times 2 Comments Data

```{r, message=F, warning=F}
sonya_nytimes2_comments <- "C:/Users/kadej/Dropbox/Quanteda/nytimes2comments.txt"
sonya_nytimes2_page_comments <- readtext(sonya_nytimes2_comments) 
print(sonya_nytimes2_page_comments)
```

#### Generate Corpus for New York Times 2 Comments

```{r, message=F, warning=F}
corpus_sonya_nytimes2_comments <- corpus(sonya_nytimes2_page_comments) #make corpus
print(corpus_sonya_nytimes2_comments)
summary(corpus_sonya_nytimes2_comments)
head(docvars(corpus_sonya_nytimes2_comments))
```

#### Create Tokens, Remove Punctuation, Numbers, and Comments

```{r, message=F, warning=F}
sonya_token_nytimes2_comments <- tokens(corpus_sonya_nytimes2_comments) #tokenized the pdf text
sonya_nonpunc_nytimes2_comments <- tokens(corpus_sonya_nytimes2_comments, remove_punct = TRUE, remove_numbers = TRUE) #remove punctuation and numbers
print(sonya_nonpunc_nytimes2_comments)
sonya_nostop_nytimes2_comments <- tokens_select(sonya_nonpunc_nytimes2_comments, pattern = stopwords("en"), selection = "remove") #remove articles
print(sonya_nostop_nytimes2_comments)
```

#### Remove Custom Words

```{r, message=F, warning=F}
nytimes2_comments_custom_stop_words <- c("reply","july","replies", "share","comments","recommended","expand_more","wrre","recommendshare","flag","commented","s","m")
sonya_custom_nytimes2_comments <- tokens_select(sonya_nonpunc_nytimes2_comments, pattern = c((stopwords("en")),(nytimes2_comments_custom_stop_words)), selection = "remove") #remove custom words
print(sonya_custom_nytimes2_comments)
```

#### Dataframe

```{r, message=F, warning=F}
sonya.dfm.nytimes2.comments <- dfm(sonya_nostop_nytimes2_comments) #put the text into a dataframe matrix
sonya.nytimes2.comments.frequency <- topfeatures(sonya.dfm.nytimes2.comments, n = 10) #asked for the top 10 words in article
print(sonya.nytimes2.comments.frequency)
sonya.nytimes2.comments.dfm.custom <- dfm(sonya_custom_nytimes2_comments)
sonya.nytimes2.comments.frequency.custom <- topfeatures(sonya.nytimes2.comments.dfm.custom, n = 10)
print(sonya.nytimes2.comments.frequency.custom)
```

#### Table
```{r, message=F, warning=F}
dataframe.nytimes2.comments <- data.frame(sonya.nytimes2.comments.frequency.custom)
colnames(dataframe.nytimes2.comments) <- "frequency"
#view(dataframe.nytimes1)
dataframe.nytimes2.comments.organized <- tibble::rownames_to_column(dataframe.nytimes2.comments, var = "topwords")
#view(dataframe.nytimes2.comments.organized)
dataframe.nytimes2.comments.categories <- dataframe.nytimes2.comments.organized %>%
  mutate(Group = case_when(
    topwords %in% c("massey", "water", "pot", "black","woman","unarmed","boiling","boil","sonya") ~ "victim",  # For A, B, C -> "first"
    topwords %in% c("grayson", "officer", "deputy","police","deputy","enforcement","officers","deputies","cop","cops","violated","policy") ~ "officer",   # For X, Y, Z -> "last"
    TRUE ~ "other"                            # Default case
  )) 
#view(dataframe.nytimes2.comments.categories)
filter(dataframe.nytimes2.comments.categories) #filter variables
#filter.nytimes2.comments.categories 
#View(filter.nytimes2.comments.categories)
```



# Table Merging

## CNN 1

```{r, message = FALSE, warning = FALSE}
summary_cnn1_article <- data.frame(dataframe.cnn1.categories %>%
  group_by(Group) %>%
  #filter(Group %in% c("victim", "officer")) %>%
  summarize(cnn1_article_frequency = sum(frequency)))
#view(summary_cnn1_article)
```

## CNN 2

```{r, message = FALSE< warning = FALSE}
summary_cnn2_article <- data.frame(dataframe.cnn2.categories %>%
  group_by(Group) %>%
  #filter(Group %in% c("victim", "officer")) %>%
  summarize(cnn2_article_frequency = sum(frequency)))
#view(summary_cnn2_article)
```

## Fox 1

```{r, message = FALSE, warning = FALSE}
summary_fox1_article <- data.frame(dataframe.fox1.categories %>%
  group_by(Group) %>%
  #filter(Group %in% c("victim", "officer")) %>%
  summarize(fox1_article_frequency = sum(frequency)))
#view(summary_fox1_article)
summary_fox1_article$percentage_fox1_article <- round((summary_fox1_article$fox1_article_frequency/sum(summary_fox1_article$fox1_article_frequency))* 100, 2)
summary_fox1_article$percentage_fox1 <- NULL
#view(summary_fox1_article)

summary_fox1_comments <- data.frame(dataframe.fox1.comments.categories %>%
  group_by(Group) %>% 
  #filter(Group %in% c("victim", "officer")) %>%
  summarize(fox1_comments_frequency = sum(frequency)))
#view(summary_fox1_comments)
summary_fox1_comments$percentage_fox1_comments <- round((summary_fox1_comments$fox1_comments_frequency/sum(summary_fox1_comments$fox1_comments_frequency))* 100, 2)

merged_fox1 <- merge(summary_fox1_article, summary_fox1_comments)
#view(merged_fox1)
```

## Fox 2
```{r, message = FALSE, warning = FALSE}
summary_fox2_article <- data.frame(dataframe.fox2.categories %>%
  group_by(Group) %>%
  #filter(Group %in% c("victim", "officer")) %>%
  summarize(fox2_article_frequency = sum(frequency)))
#view(summary_fox2_article)
summary_fox2_article$percentage_fox2_article <- round((summary_fox2_article$fox2_article_frequency/sum(summary_fox2_article$fox2_article_frequency))* 100, 2)

summary_fox2_comments <- data.frame(dataframe.fox2.comments.categories %>%
  group_by(Group) %>% 
  #filter(Group %in% c("victim", "officer")) %>%
  summarize(fox2_comments_frequency = sum(frequency)))
#view(summary_fox2_comments)
summary_fox2_comments$percentage_fox2_comments <- round((summary_fox2_comments$fox2_comments_frequency/sum(summary_fox2_comments$fox2_comments_frequency))* 100, 2)

merged_fox2 <- merge(summary_fox2_article, summary_fox2_comments)
#write.csv(merged_fox2, file = "mergedfox2.csv")
#view(merged_fox2)
```

## ABC 1

```{r, message = FALSE< warning = FALSE}
summary_abc1_article <- data.frame(dataframe.abc1.categories %>%
  group_by(Group) %>%
  #filter(Group %in% c("victim", "officer")) %>%
  summarize(abc1_article_frequency = sum(frequency)))
#view(summary_abc1_article)
```

## ABC 2

```{r, message = FALSE< warning = FALSE}
summary_abc2_article <- data.frame(dataframe.abc2.categories %>%
  group_by(Group) %>%
  #filter(Group %in% c("victim", "officer")) %>%
  summarize(abc2_article_frequency = sum(frequency)))
#view(summary_abc2_article)
```


## NBC 1

```{r, message = FALSE< warning = FALSE}
summary_nbc1_article <- data.frame(dataframe.nbc1.categories %>%
  group_by(Group) %>%
  #filter(Group %in% c("victim", "officer")) %>%
  summarize(nbc1_article_frequency = sum(frequency)))
#view(summary_nbc1_article)
```

## NBC 2

```{r, message = FALSE< warning = FALSE}
summary_nbc2_article <- data.frame(dataframe.nbc2.categories %>%
  group_by(Group) %>%
  #filter(Group %in% c("victim", "officer")) %>%
  summarize(nbc2_article_frequency = sum(frequency)))
#view(summary_nbc2_article)
```

## Washington Post 1
```{r, message = FALSE, warning = FALSE}
summary_wapo1_article <- data.frame(dataframe.wapo1.categories %>%
  group_by(Group) %>%
  #filter(Group %in% c("victim", "officer")) %>%
  summarize(wapo1_article_frequency = sum(frequency)))
#view(summary_wapo1_article)
summary_wapo1_article$percentage_wapo1_article <- round((summary_wapo1_article$wapo1_article_frequency/sum(summary_wapo1_article$wapo1_article_frequency))* 100, 2)

summary_wapo1_comments <- data.frame(dataframe.wapo1.comments.categories %>%
  group_by(Group) %>% 
  #filter(Group %in% c("victim", "officer")) %>%
  summarize(wapo1_comments_frequency = sum(frequency)))
#view(summary_wapo1_comments)
summary_wapo1_comments$percentage_wapo1_comments <- round((summary_wapo1_comments$wapo1_comments_frequency/sum(summary_wapo1_comments$wapo1_comments_frequency))* 100, 2)

merged_wapo1 <- merge(summary_wapo1_article, summary_wapo1_comments) 
#view(merged_wapo1)
```

## Washington Post 2

```{r, message = FALSE, warning = FALSE}
summary_wapo2_article <- data.frame(dataframe.wapo2.categories %>%
  group_by(Group) %>%
  #filter(Group %in% c("victim", "officer")) %>%
  summarize(wapo2_article_frequency = sum(frequency)))
#view(summary_wapo2_article)
summary_wapo2_article$percentage_wapo2_article <- round((summary_wapo2_article$wapo2_article_frequency/sum(summary_wapo2_article$wapo2_article_frequency))* 100, 2)

summary_wapo2_comments <- data.frame(dataframe.wapo2.comments.categories %>%
  group_by(Group) %>% 
  #filter(Group %in% c("victim", "officer")) %>%
  summarize(wapo2_comments_frequency = sum(frequency)))
#view(summary_wapo2_comments)
summary_wapo2_comments$percentage_wapo2_comments <- round((summary_wapo2_comments$wapo2_comments_frequency/sum(summary_wapo2_comments$wapo2_comments_frequency))* 100, 2)

merged_wapo2 <- merge(summary_wapo2_article, summary_wapo2_comments) 
#view(merged_wapo2)
```


## New York Times 1
```{r, message = FALSE, warning = FALSE}
summary_nytimes1_article <- data.frame(dataframe.nytimes1.categories %>%
  group_by(Group) %>%
  #filter(Group %in% c("victim", "officer")) %>%
  summarize(nytimes1_article_frequency = sum(frequency)))
#view(summary_nytimes1_article)
```

## New York Times 2
```{r, message = FALSE, warning = FALSE}
summary_nytimes2_article <- data.frame(dataframe.nytimes2.categories %>%
  group_by(Group) %>%
  #filter(Group %in% c("victim", "officer")) %>%
  summarize(nytimes2_article_frequency = sum(frequency)))
#view(summary_nytimes2_article)
summary_nytimes2_article$percentage_nytimes2_article <- round((summary_nytimes2_article$nytimes2_article_frequency/sum(summary_nytimes2_article$nytimes2_article_frequency))* 100, 2)

summary_nytimes2_comments <- data.frame(dataframe.nytimes2.comments.categories %>%
  group_by(Group) %>% 
  #filter(Group %in% c("victim", "officer")) %>%
  summarize(nytimes2_comments_frequency = sum(frequency)))
#view(summary_nytimes2_comments)
summary_nytimes2_comments$percentage_nytimes2_comments <- round((summary_nytimes2_comments$nytimes2_comments_frequency/sum(summary_nytimes2_comments$nytimes2_comments_frequency)) * 100, 2)

merged_nytimes2 <- merge(summary_nytimes2_article, summary_nytimes2_comments)
#merged_nytimes2
```

## Comment Merge
```{r, message = FALSE, warning = FALSE}
merged_fox <- merge(merged_fox1, merged_fox2)
merged_wapo <- merge(merged_wapo1, merged_wapo2)
merged_pair <- merge(merged_fox, merged_wapo)
merged_comments <- merge(merged_pair, merged_nytimes2)
view(merged_comments)
comments_dataframe <- as.data.frame(t(merged_comments))
colnames(comments_dataframe) <- as.character(comments_dataframe[1, ])
comments_dataframe <- comments_dataframe[-1, ]
rownames(comments_dataframe) <- case_when(rownames(comments_dataframe) == "fox1_article_frequency" ~ "Fox 1 Article",
                                          rownames(comments_dataframe) == "fox1_comments_frequency" ~ "Fox 1 Comments",
                                          rownames(comments_dataframe) == "fox2_article_frequency" ~ "Fox 2 Article",
                                          rownames(comments_dataframe) == "fox2_comments_frequency" ~ "Fox 2 Comments",
                                          rownames(comments_dataframe) == "wapo1_article_frequency" ~ "Washington Post 1 Article",
                                          rownames(comments_dataframe) == "wapo1_comments_frequency" ~ "Washington Post 1 Comments",
                                          rownames(comments_dataframe) == "wapo2_article_frequency" ~ "Washington Post 2 Article",
                                          rownames(comments_dataframe) == "wapo2_comments_frequency" ~ "Washington Post 2 Comments",
                                          rownames(comments_dataframe) == "nytimes2_article_frequency" ~ "New York Times 2 Article",
                                          rownames(comments_dataframe) == "nytimes2_comments_frequency" ~ "New York Times 2 Comments",
                                          rownames(comments_dataframe) == "percentage_fox1_article" ~ "Fox 1 Article Relative Frequency",
                                          rownames(comments_dataframe) == "percentage_fox1_comments" ~ "Fox 1 Comment Relative Frequency",
                                          rownames(comments_dataframe) == "percentage_fox2_article" ~ "Fox 2 Article Relative Frequency",
                                          rownames(comments_dataframe) == "percentage_fox2_comments" ~ "Fox 2 Comment Relative Frequency",
                                          rownames(comments_dataframe) == "percentage_wapo1_article" ~ "Washington Post 1 Article Relative Frequency",
                                          rownames(comments_dataframe) == "percentage_wapo1_comments" ~ "Washington Post 1 Comment Relative Frequency",
                                          rownames(comments_dataframe) == "percentage_wapo2_article" ~ "Washington Post 2 Article Relative Frequency",
                                          rownames(comments_dataframe) == "percentage_wapo2_comments" ~ "Washington Post 2 Comment Relative Frequency",
                                          rownames(comments_dataframe) == "percentage_nytimes2_article" ~ "New York Times 2 Article Frequency",
                                          rownames(comments_dataframe) == "percentage_nytimes2_comments" ~ "New York Times 2 Comment Relative Frequency",
                                          TRUE ~ rownames(comments_dataframe))
view(comments_dataframe)
percentage_dataframe <- comments_dataframe[-c(1,3,5,7,9,11,13,15,17,19), ]
view(percentage_dataframe)
```

## Plot

```{r, message = FALSE, warning = FALSE}
merged_comments_plot <- merged_comments[-2, ]
# Fox 1
barplot(merged_comments_plot$percentage_fox1_article,
        names.arg = merged_comments_plot$Group,
  col = "lightblue",
  main = "fox 1 article frequency percentage",
  xlab = "Groups",
  ylab = "Percentages"
)
barplot(merged_comments_plot$percentage_fox1_comments,
        names.arg = merged_comments_plot$Group,
  col = "lightblue",
  main = "fox 1 comments frequency percentage",
  xlab = "Groups",
  ylab = "Percentages"
)
# Fox 2
barplot(merged_comments_plot$percentage_fox2_article,
        names.arg = merged_comments_plot$Group,
  col = "lightblue",
  main = "fox 2 article frequency percentage",
  xlab = "Groups",
  ylab = "Percentages"
)
barplot(merged_comments_plot$percentage_fox2_comments,
        names.arg = merged_comments_plot$Group,
  col = "lightblue",
  main = "fox 2 comments frequency percentage",
  xlab = "Groups",
  ylab = "Percentages"
)
# Washington Post 1
barplot(merged_comments_plot$percentage_wapo1_article,
        names.arg = merged_comments_plot$Group,
  col = "lightblue",
  main = "washington post 1 article frequency percentage",
  xlab = "Groups",
  ylab = "Percentages"
)
barplot(merged_comments_plot$percentage_wapo1_comments,
        names.arg = merged_comments_plot$Group,
  col = "lightblue",
  main = "washington post 1 comments frequency percentage",
  xlab = "Groups",
  ylab = "Percentages"
)
```

# Chi-Squared

## Fox 1
```{r,message = FALSE, warning = FALSE}
fox1_data_c <- summary_fox1_comments %>% 
  group_by(Group) %>% 
  select(fox1_comments_frequency)
fox1_table_data_c <- table(fox1_data_c)
fox1_chi_square_test_c <- chisq.test(fox1_table_data_c)
print(fox1_chi_square_test_c)

fox1_data_a <- dataframe.fox1.categories %in% select(Group) %>% select(frequency)
fox1_table_data_a <- table(fox1_data_a$Group)
print(fox1_table_data_a)
fox1_chi_square_test_a <- chisq.test(fox1_table_data_a)
print(fox1_chi_square_test_a)
```

## Fox 2
```{r, message = FALSE, warning = FALSE}
fox2_data <- dataframe.fox2.comments.categories %>% select(Group)
fox2_table_data <- table(selected_data$Group)
print(fox2_table_data)
fox2_chi_square_test <- chisq.test(fox2_table_data)
print(fox2_chi_square_test)
```